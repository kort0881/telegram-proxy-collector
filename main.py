import requests
import re
import socket
import concurrent.futures
import time
from urllib.parse import urlparse, parse_qs
from collections import Counter, defaultdict
import logging
import json
from datetime import datetime
import ipaddress

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏
SOURCES = [
    "https://raw.githubusercontent.com/SoliSpirit/mtproto/master/proxies.txt",
    "https://raw.githubusercontent.com/hookzof/socks5_list/master/tg/mtproto.json",
    "https://raw.githubusercontent.com/soroushmirzaei/telegram-proxies-collector/main/proxies.txt",
    "https://raw.githubusercontent.com/MrPotat-00/MTProtoProxiesScraper/main/proxies.txt",
    "https://raw.githubusercontent.com/DigneZzZ/telegram-mtproto-proxies/master/proxy_list.txt",
    "https://raw.githubusercontent.com/zloi-user/hideip.me/main/proxy.txt",
    "https://raw.githubusercontent.com/ErcinDedeoglu/proxies/main/proxies/mtproto.txt",
    "https://raw.githubusercontent.com/ObcbO/getproxy/master/proxy.txt",
    "https://raw.githubusercontent.com/iw4p/MTProtoCollector/main/proxies.txt",
    "https://raw.githubusercontent.com/ALiasGHARBi/MTProtoProxies/main/proxies.txt"
]

# –§–∞–π–ª—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
OUTPUT_RU = "proxy_ru.txt"          # –†—É—Å—Å–∫–∏–µ –ø—Ä–æ–∫—Å–∏
OUTPUT_EU = "proxy_eu.txt"          # –ï–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ/–≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–æ–∫—Å–∏
OUTPUT_ALL = "proxy_all.txt"        # –í—Å–µ –ø—Ä–æ–∫—Å–∏
OUTPUT_STATS = "proxy_stats.json"   # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
TIMEOUT = 3
MAX_WORKERS = 150
MIN_RESPONSE_TIME = 0.01
MAX_RESPONSE_TIME = 2.5

# ============ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –î–û–ú–ï–ù–û–í ============

# –†—É—Å—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã –∏ —Å–µ—Ä–≤–∏—Å—ã
RU_DOMAINS = [
    # –î–æ–º–µ–Ω—ã
    '.ru', '.su', '.—Ä—Ñ', '.moscow', '.tatar',
    
    # –ö—Ä—É–ø–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
    'yandex.', 'vk.com', 'vkontakte', 'mail.ru', 'mailru',
    'sberbank', 'tinkoff', 'alfabank', 'vtb.', 'gazprom',
    'ozon.', 'wildberries', 'avito.', 'cian.', 'drom.',
    'gosuslugi', 'nalog.', 'mos.ru', 'government',
    'rzd.', 'aeroflot', 'pochta.', 's7.',
    'kaspersky', 'drweb', '1c.', 'bitrix',
    'rutube', 'okko.', 'ivi.', 'kinopoisk',
    'mts.', 'megafon', 'beeline', 'tele2',
    'lenta.', 'dns-shop', 'mvideo', 'eldorado',
    'hh.ru', 'superjob', 'rabota.',
]

# –ï–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã –∏ —Å–µ—Ä–≤–∏—Å—ã
EU_DOMAINS = [
    # –ï–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã
    '.de', '.fr', '.nl', '.uk', '.it', '.es', '.pl', '.se', 
    '.fi', '.no', '.dk', '.at', '.ch', '.be', '.cz', '.pt',
    '.ie', '.gr', '.hu', '.ro', '.bg', '.sk', '.hr', '.si',
    '.ee', '.lv', '.lt', '.lu', '.cy', '.mt',
    '.eu', '.europa',
    
    # –ï–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ —Å–µ—Ä–≤–∏—Å—ã –∏ –∫–æ–º–ø–∞–Ω–∏–∏
    'hetzner', 'ovh.', 'scaleway', 'contabo',
    'deutsche', 'telefonica', 'orange.', 'vodafone',
    'bbc.', 'guardian', 'spiegel', 'lemonde',
    'airbus', 'siemens', 'bosch', 'bmw', 'mercedes',
    'spotify', 'klarna', 'adyen', 'booking',
]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ/CDN –¥–æ–º–µ–Ω—ã (–±—É–¥—É—Ç –≤ EU –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
GLOBAL_DOMAINS = [
    # CDN –∏ –æ–±–ª–∞–∫–∞
    'cloudflare', 'fastly', 'akamai', 'cloudfront', 'azurefd',
    'googleapis', 'googleusercontent', 'gstatic', 'fbcdn',
    'amazon', 'aws.', 'azure.', 'digitalocean',
    
    # Tech –≥–∏–≥–∞–Ω—Ç—ã
    'google.com', 'microsoft.com', 'apple.com', 'meta.com',
    'facebook.com', 'netflix.com', 'github.', 'gitlab',
    'twitter.com', 'x.com', 'instagram', 'whatsapp',
    'zoom.', 'slack.', 'discord.',
    
    # –ü—Ä–æ—á–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ
    'wikipedia', 'reddit.', 'stackoverflow',
]

# –ö—ç—à –∏ blacklist
checked_hosts = {}
blacklist_hosts = set()

class ProxyClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–∫—Å–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º"""
    
    @staticmethod
    def decode_secret_domain(secret):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ Fake-TLS —Å–µ–∫—Ä–µ—Ç–∞"""
        if not secret or not secret.startswith('ee'):
            return None
        try:
            hex_part = secret[2:]
            if len(hex_part) % 2 != 0:
                hex_part = hex_part[:-1]
            
            # –ü–æ–∏—Å–∫ null-–±–∞–π—Ç–∞
            for i in range(0, len(hex_part), 2):
                if hex_part[i:i+2] == '00':
                    hex_part = hex_part[:i]
                    break
            
            domain_bytes = bytes.fromhex(hex_part)
            domain = domain_bytes.decode('utf-8', errors='ignore')
            domain = ''.join(c for c in domain if c.isprintable())
            
            return domain.lower() if domain else None
        except:
            return None
    
    @staticmethod
    def classify_by_domain(domain):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø—Ä–æ–∫—Å–∏ –ø–æ –¥–æ–º–µ–Ω—É –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏"""
        if not domain:
            return 'unknown'
        
        domain_lower = domain.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ RU
        for ru_pattern in RU_DOMAINS:
            if ru_pattern in domain_lower:
                return 'ru'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ EU
        for eu_pattern in EU_DOMAINS:
            if eu_pattern in domain_lower:
                return 'eu'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Global (–æ—Ç–Ω–æ—Å–∏–º –∫ EU)
        for global_pattern in GLOBAL_DOMAINS:
            if global_pattern in domain_lower:
                return 'eu'
        
        return 'other'
    
    @staticmethod
    def get_quality_score(secret, region):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏–æ–Ω–∞"""
        score = 0
        
        if secret.startswith("ee"):
            score += 20
            domain = ProxyClassifier.decode_secret_domain(secret)
            
            if domain:
                # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–µ–≥–∏–æ–Ω—É
                if region == 'ru':
                    for ru_pattern in RU_DOMAINS[:15]:  # –¢–æ–ø RU —Å–µ—Ä–≤–∏—Å—ã
                        if ru_pattern in domain.lower():
                            score += 50
                            break
                elif region == 'eu':
                    for eu_pattern in EU_DOMAINS + GLOBAL_DOMAINS:
                        if eu_pattern in domain.lower():
                            score += 30
                            break
                
                if len(secret) > 32:
                    score += 10
                    
        elif secret.startswith("dd"):
            score += 10
            if len(secret) == 34:
                score += 5
        
        return score

def parse_proxy(line):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ø—Ä–æ–∫—Å–∏"""
    line = line.strip()
    if not line or line.startswith('#'):
        return None
    
    patterns = [
        r'(?:tg://|https?://t\.me/)proxy\?server=([^&]+)&port=(\d+)&secret=([a-fA-F0-9]+)',
        r'["\'](?:server|host)["\']:\s*["\']([^"\']+)["\'].*?["\']port["\']:\s*(\d+).*?["\']secret["\']:\s*["\']([a-fA-F0-9]+)',
        r'^([a-zA-Z0-9\.\-]+):(\d+):([a-fA-F0-9]+)$',
        r'([a-zA-Z0-9\.\-]+)\|(\d+)\|([a-fA-F0-9]+)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, line, re.IGNORECASE)
        if match:
            try:
                server = match.group(1)
                port = int(match.group(2))
                secret = match.group(3)
                if 1 <= port <= 65535 and len(secret) >= 4:
                    return server, port, secret
            except:
                continue
    
    return None

def check_proxy(proxy_data):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏"""
    host, port, secret = proxy_data
    
    # –ö—ç—à
    cache_key = f"{host}:{port}"
    if cache_key in checked_hosts:
        cached = checked_hosts[cache_key]
        if time.time() - cached['time'] < 300:
            return cached['result']
    
    # Blacklist
    if host in blacklist_hosts:
        return None
    
    # –§–∏–ª—å—Ç—Ä —Å–µ–∫—Ä–µ—Ç–∞
    if not secret or len(secret) < 4:
        return None
    if not (secret.startswith("ee") or secret.startswith("dd")):
        return None
    
    # DNS
    try:
        socket.gethostbyname(host)
    except:
        blacklist_hosts.add(host)
        return None
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä—Ç–∞
    try:
        start_time = time.time()
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(TIMEOUT)
        result = sock.connect_ex((host, port))
        response_time = time.time() - start_time
        sock.close()
        
        if result != 0:
            return None
        if response_time < MIN_RESPONSE_TIME or response_time > MAX_RESPONSE_TIME:
            return None
    except:
        return None
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    domain = ProxyClassifier.decode_secret_domain(secret)
    region = ProxyClassifier.classify_by_domain(domain)
    quality_score = ProxyClassifier.get_quality_score(secret, region)
    
    result = {
        'link': f"tg://proxy?server={host}&port={port}&secret={secret}",
        'host': host,
        'port': port,
        'secret': secret,
        'response_time': response_time,
        'quality_score': quality_score,
        'domain': domain or 'unknown',
        'region': region,
        'timestamp': time.time()
    }
    
    checked_hosts[cache_key] = {'result': result, 'time': time.time()}
    return result

def process_source(url):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    proxies = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/plain, application/json, */*',
        }
        
        resp = requests.get(url, timeout=20, headers=headers)
        resp.raise_for_status()
        content = resp.text
        
        # JSON
        if url.endswith('.json') or 'application/json' in resp.headers.get('content-type', ''):
            try:
                data = json.loads(content)
                if isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict):
                            host = item.get('host') or item.get('server')
                            port = item.get('port')
                            secret = item.get('secret')
                            if host and port and secret:
                                proxies.append((host, int(port), secret))
            except:
                pass
        
        # –¢–µ–∫—Å—Ç
        for line in content.splitlines():
            parsed = parse_proxy(line)
            if parsed:
                proxies.append(parsed)
        
        if proxies:
            logger.info(f"‚úì {url}: {len(proxies)} –ø—Ä–æ–∫—Å–∏")
            
    except Exception as e:
        logger.warning(f"‚úó {url}: {e}")
    
    return proxies

def save_proxies(proxies, filename, category_name):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–∞–π–ª"""
    if not proxies:
        logger.warning(f"‚ö† {category_name}: –Ω–µ—Ç –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# {category_name} MTProto Proxies\n")
        f.write(f"# –û–±–Ω–æ–≤–ª–µ–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(proxies)}\n")
        f.write("#" + "=" * 50 + "\n\n")
        
        for p in proxies:
            f.write(p['link'] + "\n")
    
    logger.info(f"üíæ {category_name}: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(proxies)} –≤ {filename}")

def main():
    start_time = time.time()
    
    print()
    print("=" * 60)
    print("üöÄ MTProto Proxy Collector - RU/EU Edition")
    print("=" * 60)
    print()
    
    # === –≠–¢–ê–ü 1: –°–ë–û–† ===
    logger.info(f"üì° –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ {len(SOURCES)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    all_candidates = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(process_source, url) for url in SOURCES]
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            if result:
                all_candidates.extend(result)
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
    unique_map = {}
    for proxy in all_candidates:
        key = f"{proxy[0]}:{proxy[1]}"
        if key not in unique_map:
            unique_map[key] = proxy
    
    candidates = list(unique_map.values())
    logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates)}")
    
    # === –≠–¢–ê–ü 2: –ü–†–û–í–ï–†–ö–ê ===
    logger.info(f"‚ö° –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ {MAX_WORKERS} –ø–æ—Ç–æ–∫–æ–≤...")
    
    valid_proxies = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(check_proxy, p): p for p in candidates}
        
        completed = 0
        total = len(futures)
        
        for future in concurrent.futures.as_completed(futures):
            completed += 1
            if completed % 50 == 0 or completed == total:
                pct = completed * 100 // total
                print(f"\r‚è≥ –ü—Ä–æ–≥—Ä–µ—Å—Å: {completed}/{total} ({pct}%)", end='', flush=True)
            
            try:
                result = future.result(timeout=TIMEOUT + 1)
                if result:
                    valid_proxies.append(result)
            except:
                continue
    
    print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
    
    if not valid_proxies:
        logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏!")
        return
    
    # === –≠–¢–ê–ü 3: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø ===
    ru_proxies = [p for p in valid_proxies if p['region'] == 'ru']
    eu_proxies = [p for p in valid_proxies if p['region'] in ('eu', 'other', 'unknown')]
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
    ru_proxies.sort(key=lambda x: (-x['quality_score'], x['response_time']))
    eu_proxies.sort(key=lambda x: (-x['quality_score'], x['response_time']))
    valid_proxies.sort(key=lambda x: (-x['quality_score'], x['response_time']))
    
    # === –≠–¢–ê–ü 4: –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
    print()
    print("=" * 60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 60)
    
    print(f"\nüá∑üá∫ –†–£–°–°–ö–ò–ï –ø—Ä–æ–∫—Å–∏: {len(ru_proxies)}")
    if ru_proxies:
        avg_ping_ru = sum(p['response_time'] for p in ru_proxies) / len(ru_proxies)
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –ø–∏–Ω–≥: {avg_ping_ru:.3f}—Å")
        
        ru_domains = Counter(p['domain'] for p in ru_proxies if p['domain'] != 'unknown')
        if ru_domains:
            print("   ‚Ä¢ –¢–æ–ø –¥–æ–º–µ–Ω—ã:")
            for domain, count in ru_domains.most_common(5):
                print(f"     - {domain}: {count}")
    
    print(f"\nüá™üá∫ –ï–í–†–û–ü–ï–ô–°–ö–ò–ï/–ì–õ–û–ë–ê–õ–¨–ù–´–ï –ø—Ä–æ–∫—Å–∏: {len(eu_proxies)}")
    if eu_proxies:
        avg_ping_eu = sum(p['response_time'] for p in eu_proxies) / len(eu_proxies)
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –ø–∏–Ω–≥: {avg_ping_eu:.3f}—Å")
        
        eu_domains = Counter(p['domain'] for p in eu_proxies if p['domain'] != 'unknown')
        if eu_domains:
            print("   ‚Ä¢ –¢–æ–ø –¥–æ–º–µ–Ω—ã:")
            for domain, count in eu_domains.most_common(5):
                print(f"     - {domain}: {count}")
    
    print(f"\nüìà –í–°–ï–ì–û —Ä–∞–±–æ—á–∏—Ö: {len(valid_proxies)}")
    
    # === –≠–¢–ê–ü 5: –°–û–•–†–ê–ù–ï–ù–ò–ï ===
    print()
    print("=" * 60)
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï")
    print("=" * 60)
    
    save_proxies(ru_proxies, OUTPUT_RU, "üá∑üá∫ RU Proxies")
    save_proxies(eu_proxies, OUTPUT_EU, "üá™üá∫ EU/Global Proxies")
    save_proxies(valid_proxies, OUTPUT_ALL, "üìã All Proxies")
    
    # JSON —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        'updated': datetime.now().isoformat(),
        'total': len(valid_proxies),
        'ru_count': len(ru_proxies),
        'eu_count': len(eu_proxies),
        'ru_avg_ping': round(sum(p['response_time'] for p in ru_proxies) / len(ru_proxies), 3) if ru_proxies else 0,
        'eu_avg_ping': round(sum(p['response_time'] for p in eu_proxies) / len(eu_proxies), 3) if eu_proxies else 0,
        'ru_top_domains': dict(Counter(p['domain'] for p in ru_proxies if p['domain'] != 'unknown').most_common(10)),
        'eu_top_domains': dict(Counter(p['domain'] for p in eu_proxies if p['domain'] != 'unknown').most_common(10)),
        'sources_checked': len(SOURCES),
        'execution_time': round(time.time() - start_time, 2)
    }
    
    with open(OUTPUT_STATS, "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {OUTPUT_STATS}")
    
    print()
    print("=" * 60)
    print(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time.time() - start_time:.2f}—Å")
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
